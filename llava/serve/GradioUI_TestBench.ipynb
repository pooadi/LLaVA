{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8s6MfKQ0Izv9"},"outputs":[],"source":["!pip install gradio\n","!pip install Pillow\n","!pip install datasets\n","!pip install llm-lens\n","!pip install torch\n","!pip install torchvision\n","!pip install SpeechRecognition\n","!pip install moviepy\n","!pip install opencv-python\n","!pip install salesforce-lavis\n","!pip install numpy\n","!pip install ninja\n","!pip install sentencepiece\n","!pip install icecream\n","!pip install transformers==4.28.1\n","!pip install tqdm\n","!pip install decord==0.6.0\n","!pip install timm==0.6.7\n","!pip install oss2\n","!pip install markdown2\n","!pip install hjson\n","!pip install einops\n","!pip install wget\n","!pip install accelerate\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"markdown","source":["####Gradio LENS"],"metadata":{"id":"l4drlTkXyhOn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2qMw3hvq-Eb"},"outputs":[],"source":["\"\"\"\n","Python file that stores the refactored classes for LENS and BLIP_VQA to make the app.py much cleaner\n","Author: Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","import re\n","import torch\n","from lavis.models import load_model_and_preprocess\n","from lens import Lens, LensProcessor\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","\n","########################################################################################################################\n","\n","class LENSInference:\n","    \"\"\"\n","    class for running LENs Inference\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialisation function of the class\n","        \"\"\"\n","        self._lens = Lens()\n","        self._processor = LensProcessor()\n","\n","        # regex to remove the <pad> and </s> from the output of the LLMs\n","        self._remWord1 = re.compile('(\\s*)<pad>(\\s*)')\n","        self._remWord2 = re.compile('(\\s*)</s>(\\s*)')\n","\n","    ####################################################################################################################\n","\n","    def __call__(\n","            self,\n","            imageFrame,\n","            question\n","    ):\n","        \"\"\"\n","        function to be called to run the inference\n","        :param imageFrame: List of image to be inferred\n","        :param question: List of questions to be passed into the network\n","        :return: [lLMOutput, ImageCaption] : Output of LLM (LENS) + image caption\n","        \"\"\"\n","\n","        # inferring the initial vision models like BLIP, CLIP\n","\n","        samples = self._processor(imageFrame, question)\n","        outputInit = self._lens(samples)\n","\n","        # feeding the output of the vision models to a frozen LLM\n","        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", truncation_side='left', padding=True)\n","        lLMModel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n","        inputIds = tokenizer(samples[\"prompts\"], return_tensors=\"pt\").input_ids\n","        outputLLM = lLMModel.generate(inputIds)\n","        lLMOutput = str(tokenizer.decode(outputLLM[0]))\n","\n","        # use regex to remove unnecessary prefix and suffix\n","        lLMOutput = self._remWord1.sub('', lLMOutput)\n","        lLMOutput = self._remWord2.sub('', lLMOutput)\n","\n","        return [lLMOutput, outputInit[\"caption\"][0]]\n","\n","\n","########################################################################################################################\n","\n","class BLIPVQAInference:\n","    \"\"\"\n","    class for running BLIP_VQA inference\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialisation function of the class\n","        \"\"\"\n","        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        # Loading the BLIP VQA model\n","        self._modelBLIPVQA, \\\n","            self._visProcessors, \\\n","            self._txtProcessors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\",\n","                                                            is_eval=True, device=str(self._device))\n","\n","    ################################################################################################################\n","\n","    def __call__(\n","            self,\n","            imageFrame,\n","            question):\n","        \"\"\"\n","        function to be called to run the inference\n","        :param imageFrame: Image to be inferred\n","        :param question: Question to be passed into the network\n","        :return: outputBLIPVQA : Output of the BLIP_VQA\n","        \"\"\"\n","\n","        imageBLIPVQA = self._visProcessors[\"eval\"](imageFrame).unsqueeze(0).to(self._device)\n","\n","        questionBLIPVQA = self._txtProcessors[\"eval\"](question)\n","\n","        # blip VQA output\n","        bLIPVQAOutput = self._modelBLIPVQA.predict_answers(\n","            samples={\"image\": imageBLIPVQA, \"text_input\": questionBLIPVQA},\n","            inference_method=\"generate\")\n","\n","        return bLIPVQAOutput[0]\n","\n","########################################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpJmJkRBHeSi"},"outputs":[],"source":["\"\"\"\n","This code is written and maintained by Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","import torch\n","import gradio as gr\n","import decord\n","import numpy as np\n","from PIL import Image\n","#from . import dirPath\n","\n","\n","########################################################################################################################\n","class DemoGUIGradio:\n","\n","    ########################################## - INITIALIZE - ##########################################################\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialisation function\n","        \"\"\"\n","        # Load the first model\n","        self._model1 = LENSInference()\n","\n","        # Loading the second model\n","        self._model2 = BLIPVQAInference()\n","\n","    ######################################- GRADIO BACKEND FUNCTIONS - #################################################\n","\n","    @staticmethod\n","    def _getFrameIds(startFrame,\n","                     endFrame,\n","                     numSegments=32,\n","                     jitter=True):\n","\n","        segSize = float(endFrame - startFrame - 1) / numSegments\n","        seq = []\n","\n","        for i in range(numSegments):\n","\n","            start = int(np.round(segSize * i) + startFrame)\n","            end = int(np.round(segSize * (i + 1)) + startFrame)\n","            end = min(end, endFrame)\n","            if jitter:\n","                frameId = np.random.randint(low=start, high=(end + 1))\n","            else:\n","                frameId = (start + end) // 2\n","            seq.append(frameId)\n","\n","        return seq\n","\n","    ####################################################################################################################\n","\n","    def _imageInference(self,\n","                        imageFrame,\n","                        question):\n","        \"\"\"\n","        Function to run when you click on the submit button\n","\n","        :param imageFrame: Image uploaded by the user\n","        :param question: question asked by the user\n","        :return: [LENS Answer, BLIP VQA Answer, Image Description]\n","\n","        \"\"\"\n","\n","        if question == \"\":\n","            question = \"What is the sentiment expressed in the image?\"\n","\n","        model1Output = [\"\", \"\"]\n","\n","        with torch.no_grad():\n","\n","            # inference of Model 2\n","            model2Output = self._model2(imageFrame, question)\n","\n","            # inference of Model 1\n","            model1Output = self._model1([imageFrame], [question])\n","\n","        return [str(model1Output[0]), str(model2Output), str(model1Output[1])]\n","\n","    ######################################- GRADIO BACKEND FUNCTIONS - #################################################\n","\n","    def _videoInference(self,\n","                        videoFile,\n","                        question,\n","                        numSegments=4,\n","                        strideSize=16):\n","        \"\"\"\n","        Function to run when you click on the submit button\n","        :param videoFile: Video uploaded by the user\n","        :param question: question asked by the user\n","        :param numSegments:\n","        :param strideSize:\n","        :return: [LENS Answer, BLIP VQA Answer, Image Description]\n","        \"\"\"\n","\n","        if question == \"\":\n","            question = \"What is the sentiment expressed in the image?\"\n","\n","        if numSegments == \"\":\n","          numSegments = 4\n","\n","        if strideSize ==\"\":\n","          strideSize = 16\n","\n","        model1Output = \"\"\n","        model2Output = \"\"\n","        imageDescription = \"\"\n","\n","        inputVideo = decord.VideoReader(videoFile)\n","        frameSampleSize = int(numSegments) * int(strideSize)\n","        maxStartFrame = len(inputVideo) - frameSampleSize\n","        fps = inputVideo.get_avg_fps()\n","        currFrame = 0\n","\n","        while currFrame == 0 or currFrame < maxStartFrame:\n","\n","            stopFrame = min(currFrame + frameSampleSize, len(inputVideo))\n","            currSec, stopSec = currFrame / fps, stopFrame / fps\n","            frameIds = self._getFrameIds(currFrame, stopFrame, numSegments=numSegments, jitter=False)\n","            frames = inputVideo.get_batch(frameIds).asnumpy()\n","            textToAdd = f\"{'-' * 30} Predictions From: {currSec:2.3f}-{stopSec:2.3f} seconds {'-' * 30}\\n\"\n","            model1Output += textToAdd\n","            model2Output += textToAdd\n","            imageDescription += textToAdd\n","            i = 0\n","\n","            for frame in frames:\n","\n","                imageFrame = Image.fromarray(frame, \"RGB\")\n","\n","                with torch.no_grad():\n","\n","                    # inference of model 2\n","                    _model2Output = self._model2(imageFrame, question)\n","                    textToAdd = f\"Frame {frameIds[i]}: {_model2Output}\\n\"\n","                    model2Output += textToAdd\n","\n","                    # inference of Model 1\n","                    _model1Output = self._model1([imageFrame], [question])\n","                    textToAdd = f\"Frame {frameIds[i]}: {_model1Output[0]}\\n\"\n","                    model1Output += textToAdd\n","                    textToAdd = f\"Frame {frameIds[i]}: {_model1Output[1]}\\n\"\n","                    imageDescription += textToAdd\n","\n","                i += 1\n","            currFrame += frameSampleSize\n","\n","        return [str(model1Output), str(model2Output), str(imageDescription)]\n","        #return model2Output\n","\n","    ######################################- GRADIO FRONTEND FUNCTIONS - ################################################\n","\n","    def __call__(self):\n","        \"\"\"\n","        Function that creates the frontend Gradio interface\n","        \"\"\"\n","\n","        # Creating a block for the app\n","        with gr.Blocks(title=\"Demo for Sentiment Detection using Multimodal LLMs\") as interface:\n","\n","            # title of the demo\n","            # title = \"Demo for Sentiment Detection using Multimodal LLMs\"\n","\n","            # description of the demo\n","            description = \"Gradio initial demo for a proposed Sentiment/Emotion Detection using Multimodal LLM\"\n","\n","            # example Files\n","\n","            \"\"\"exampleInputs = [[f\"{dirPath}/Examples/Crying1.jpeg\", \"What is the facial expression of the person in the \"\n","                                                                  \"image?\"],\n","                             [f\"{dirPath}/Examples/Crying2.jpeg\", \"Describe the emotion expressed in the image\"],\n","                             [f\"{dirPath}/Examples/Happy1.jpeg\", \"What is the person expressing in the image?\"],\n","                             [f\"{dirPath}/Examples/Happy2.jpeg\", \"What is the sentiment expressed in the image?\"],\n","                             [f\"{dirPath}/Examples/SadGirl.jpeg\", \"What is the person doing in the image?\"]]\n","\n","            exampleVideoInputs = [[f\"{dirPath}/Examples/Example1.mp4\",\n","                                   \"What is the facial expression of the person in the \"\n","                                                                  \"image?\"]]\"\"\"\n","\n","            # Setting up Markdown for Title and Description\n","            # gr.Markdown(value=f\"# <p style=\\\"text-align: center;\\\"> {title} </p>\")\n","            gr.Markdown(value=f\"#### {description}\")\n","\n","            with gr.Tab(\"Image File\"):\n","                # Image Tab\n","\n","                with gr.Row():\n","\n","                    with gr.Column():\n","\n","                        # The input components list\n","                        inputs = [gr.Image(type='pil', interactive=True),\n","                                  gr.Textbox(lines=2, label=\"Question\",\n","                                             placeholder=\"Type your question here (Default question:\"\n","                                                         \" What is the sentiment expressed in the image?)...\")]\n","                        with gr.Row():\n","\n","                            # The clear and the submit button objects\n","                            clearButton = gr.ClearButton()\n","                            submitButton = gr.Button(value=\"Submit\", variant=\"primary\")\n","\n","                    with gr.Column():\n","\n","                        # The output components list\n","                        outputs = [gr.Textbox(label=\"Model 1 Answer\"),\n","                                   gr.Textbox(label=\"Model 2 Answer\"),\n","                                   gr.Textbox(label=\"Image Description\")]\n","\n","                # Adding components for the clear Button to clear when it is clicked\n","                clearButton.add(components=inputs + outputs)\n","\n","                # Adding the details for the submit button click action\n","                submitButton.click(fn=self._imageInference, inputs=inputs, outputs=outputs)\n","\n","                # setting up examples\n","                #examples = gr.Examples(examples=exampleInputs, inputs=inputs, outputs=outputs, fn=self._imageInference,\n","                #                       cache_examples=True)\n","\n","            with gr.Tab(\"Video File\"):\n","\n","                with gr.Row():\n","\n","                    with gr.Column():\n","\n","                        # The input components list\n","                        inputs = [gr.Video(label=\"Video File\"),\n","                                  gr.Textbox(lines=2, label=\"Question\",\n","                                             placeholder=\"Type your question here (Default question:\"\n","                                                         \" What is the sentiment expressed in the image?)...\")]\n","                        with gr.Row():\n","\n","                            # the secondary inputs\n","                            inputs2 = [gr.Textbox(label=\"Number of Segments\",\n","                                                  placeholder=\"Enter an integer value (Default: 4)\"),\n","                                       gr.Textbox(label=\"Stride Size\",\n","                                                  placeholder=\"Enter an integer value (Default: 16)\")]\n","                        with gr.Row():\n","\n","                            # The clear and the submit button objects\n","                            clearButton = gr.ClearButton()\n","                            submitButton = gr.Button(value=\"Submit\", variant=\"primary\")\n","\n","                    with gr.Column():\n","\n","                        # The output components list\n","                        outputs = [gr.Textbox(label=\"Model 1 Answer\", max_lines=5),\n","                                   gr.Textbox(label=\"Model 2 Answer\", max_lines=5),\n","                                   gr.Textbox(label=\"Description\", max_lines=5)]\n","                        #outputs = gr.Textbox(label=\"Image Description\")\n","\n","                    # Adding components for the clear Button to clear when it is clicked\n","                clearButton.add(components=inputs + outputs)\n","\n","                # Adding the details for the submit button click action\n","                submitButton.click(fn=self._videoInference, inputs=inputs + inputs2, outputs=outputs)\n","\n","                # setting up examples\n","                #examples = gr.Examples(examples=exampleVideoInputs, inputs=inputs, outputs=outputs,\n","                #                       fn=self._videoInference,\n","                #                       cache_examples=True)\n","\n","        # Launch interface\n","        interface.launch(share=True, debug=True)\n","\n","########################################################################################################################\n","\n","# Load gradio class\n","gradioUI = DemoGUIGradio()\n","\n","\n","######################################- GRADIO UI DEPLOYMENT - #####################################################\n","\n","# deploy UI\n","gradioUI()\n","\n","########################################################################################################################\n"]},{"cell_type":"markdown","metadata":{"id":"F0fxNgz24faA"},"source":["#### Gradio Chatbot UI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmpvqE6alI6A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAoGsq2F4jN4"},"outputs":[],"source":["# @title MPLUG-OWL (a lighter LLaVA for testing purposes)\n","\"\"\"\n","This code is written and maintained by Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","\n","# code to mount my drive\n","!nvidia-smi\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/mPLUG-Owl\n","\n","import os\n","import argparse\n","import datetime\n","import json\n","import os\n","import time\n","import torch\n","import gradio as gr\n","import requests\n","\n","from serve.conversation import default_conversation\n","from serve.gradio_css import code_highlight_css\n","from serve.gradio_patch import Chatbot as grChatbot\n","from serve.serve_utils import (\n","    add_text, after_process_image, disable_btn, no_change_btn,\n","    downvote_last_response, enable_btn, flag_last_response,\n","    get_window_url_params, init, regenerate, upvote_last_response,\n",")\n","from serve.model_worker import mPLUG_Owl_Server\n","from serve.model_utils import post_process_code\n","\n","########################################################################################################################\n","class DemoGUIGradio:\n","\n","    @staticmethod\n","    def loadInterface(state, request: gr.Request):\n","\n","      state = default_conversation.copy()\n","\n","      return (state)\n","\n","    ####################################################################################################################\n","\n","    @staticmethod\n","    def clearHistory(state, request: gr.Request):\n","\n","      state = default_conversation.copy()\n","\n","      return (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","    ####################################################################################################################\n","\n","    @staticmethod\n","    def addTextHttpBot( state, text, image, video, max_output_tokens, temperature, top_k, top_p,\n","                       num_beams, no_repeat_ngram_size, length_penalty,\n","                        do_sample, request: gr.Request):\n","\n","      if len(text) <= 0 and (image is None or video is None):\n","\n","        state.skip_next = True\n","        return (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","      if image is not None:\n","\n","        if '<image>' not in text:\n","\n","            text = text + '\\n<image>'\n","\n","        text = (text, image)\n","\n","      if video is not None:\n","\n","        num_frames = 4\n","\n","        if '<image>' not in text:\n","\n","          text = text + '\\n<image>' * num_frames\n","\n","        text = (text, video)\n","\n","      state.append_message(state.roles[0], text)\n","      state.append_message(state.roles[1], None)\n","      state.skip_next = False\n","\n","      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","      if state.skip_next:\n","        # This generate call is skipped due to invalid inputs\n","        yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","        return\n","\n","      prompt = after_process_image(state.get_prompt())\n","      images = state.get_images()\n","\n","      data = {\n","          \"text_input\": prompt,\n","          \"images\": images if len(images) > 0 else [],\n","          \"generation_config\": {\n","              \"top_k\": int(top_k),\n","              \"top_p\": float(top_p),\n","              \"num_beams\": int(num_beams),\n","              \"no_repeat_ngram_size\": int(no_repeat_ngram_size),\n","              \"length_penalty\": float(length_penalty),\n","              \"do_sample\": bool(do_sample),\n","              \"temperature\": float(temperature),\n","              \"max_new_tokens\": min(int(max_output_tokens), 1536),\n","              }\n","          }\n","      state.messages[-1][-1] = \"▌\"\n","      yield (state, state.to_gradio_chatbot(), \"\",None, None)\n","\n","      try:\n","          for chunk in model.predict(data):\n","              if chunk:\n","                  if chunk[1]:\n","                      output = chunk[0].strip()\n","                      output = post_process_code(output)\n","                      state.messages[-1][-1] = output + \"▌\"\n","                      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","                  else:\n","                      output = chunk[0].strip()\n","                      state.messages[-1][-1] = output\n","                      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","                      return\n","                  time.sleep(0.03)\n","\n","      except requests.exceptions.RequestException as e:\n","          state.messages[-1][-1] = \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n","          yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","          return\n","\n","      state.messages[-1][-1] = state.messages[-1][-1][:-1]\n","      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","    ######################################- GRADIO FRONTEND FUNCTIONS - ################################################\n","\n","    def __call__(self):\n","        \"\"\"\n","        Function that creates the frontend Gradio interface\n","        \"\"\"\n","\n","        # Creating a block for the app\n","        with gr.Blocks(title=\"Demo for Sentiment Detection using multimodal LLMs\") as interface:\n","\n","            state = gr.State()\n","\n","            # title of the demo\n","            title = \"Demo for Sentiment Detection using Multimodal LLMs\"\n","\n","            # description of the demo\n","            description = \"Gradio initial demo for a proposed Sentiment/Emotion Detection using Multimodal LLM\"\n","\n","            # Setting up Markdown for Title and Description\n","            gr.Markdown(value=f\"# <p style=\\\"text-align: center;\\\"> {title} </p>\")\n","            gr.Markdown(value=f\"#### {description}\")\n","\n","            with gr.Row():\n","              with gr.Column():\n","\n","                imagebox = gr.Image(type=\"pil\", visible=True)\n","                videobox = gr.Video()\n","\n","                with gr.Accordion(\"Parameters\", open=True, visible=True) as parameterRow:\n","\n","                    max_output_tokens = gr.Slider(minimum=0, maximum=1024, value=512, step=64, interactive=True, label=\"Max output tokens\",)\n","                    temperature = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, interactive=True, label=\"Temperature\",)\n","                    top_k = gr.Slider(minimum=1, maximum=5, value=3, step=1, interactive=True, label=\"Top K\",)\n","                    top_p = gr.Slider(minimum=0, maximum=1, value=0.9, step=0.1, interactive=True, label=\"Top p\",)\n","                    length_penalty = gr.Slider(minimum=1, maximum=5, value=1, step=0.1, interactive=True, label=\"length_penalty\",)\n","                    num_beams = gr.Slider(minimum=1, maximum=5, value=1, step=1, interactive=True, label=\"Beam Size\",)\n","                    no_repeat_ngram_size = gr.Slider(minimum=1, maximum=5, value=2, step=1, interactive=True, label=\"no_repeat_ngram_size\",)\n","                    do_sample = gr.Checkbox(interactive=True, value=True, label=\"do_sample\")\n","\n","              with gr.Column():\n","                chatbot = grChatbot(elem_id=\"chatbot\", visible=True).style(height=800)\n","                with gr.Row():\n","                  with gr.Column(scale=8):\n","                    textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", visible=True).style(container=False)\n","                  with gr.Column(scale=1, min_width=60):\n","                    submitBtn = gr.Button(value=\"Submit\", visible=True)\n","                with gr.Row(visible=True) as buttonRow:\n","                  clearBtn = gr.Button(value=\"🗑️  Clear history\", interactive=True)\n","\n","            btnList = [clearBtn]\n","\n","            parameter_list = [max_output_tokens, temperature, top_k,\n","                              top_p, num_beams, no_repeat_ngram_size, length_penalty, do_sample]\n","\n","            clearBtn.click(self.clearHistory, None, [state, chatbot, textbox, videobox])\n","\n","            textbox.submit(self.addTextHttpBot,\n","             [state, textbox, imagebox, videobox] + parameter_list,\n","             [state, chatbot, textbox, imagebox, videobox]\n","                           )\n","\n","            submitBtn.click(self.addTextHttpBot,\n","               [state, textbox, imagebox, videobox] + parameter_list,\n","               [state, chatbot, textbox, imagebox, videobox]\n","             )\n","\n","            interface.load(self.loadInterface, [state], [state])\n","\n","        # Launch interface\n","        #interface.launch(share=True, debug=True)\n","        return interface\n","\n","########################################################################################################################\n","\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","else:\n","  device = \"cpu\"\n","\n","model = mPLUG_Owl_Server(\n","    base_model='MAGAer13/mplug-owl-llama-7b',\n","    load_in_8bit=True,\n","    bf16=True,\n","    device=device,\n","    )\n","\n","# deploy UI\n","gradioUI = DemoGUIGradio()\n","interface = gradioUI()\n","interface.queue(concurrency_count=3, status_update_rate=10, api_open=False).launch(debug=True, share=True)\n","\n","########################################################################################################################\n","\n"]},{"cell_type":"code","source":["# @title LLaVA\n","\"\"\"\n","This code is written and maintained by Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","\n","# code to mount my drive\n","!nvidia-smi\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ChatAI_Project/LLaVA\n"],"metadata":{"id":"UfnZBQaY3NbO"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["l4drlTkXyhOn"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}