{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8s6MfKQ0Izv9"},"outputs":[],"source":["!pip install gradio\n","!pip install Pillow\n","!pip install datasets\n","!pip install llm-lens\n","!pip install torch\n","!pip install torchvision\n","!pip install SpeechRecognition\n","!pip install moviepy\n","!pip install opencv-python\n","!pip install salesforce-lavis\n","!pip install numpy\n","!pip install ninja\n","!pip install sentencepiece\n","!pip install icecream\n","!pip install transformers==4.28.1\n","!pip install tqdm\n","!pip install decord==0.6.0\n","!pip install timm==0.6.7\n","!pip install oss2\n","!pip install markdown2\n","!pip install hjson\n","!pip install einops\n","!pip install wget\n","!pip install accelerate\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"markdown","source":["####Gradio LENS"],"metadata":{"id":"l4drlTkXyhOn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2qMw3hvq-Eb"},"outputs":[],"source":["\"\"\"\n","Python file that stores the refactored classes for LENS and BLIP_VQA to make the app.py much cleaner\n","Author: Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","import re\n","import torch\n","from lavis.models import load_model_and_preprocess\n","from lens import Lens, LensProcessor\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","\n","########################################################################################################################\n","\n","class LENSInference:\n","    \"\"\"\n","    class for running LENs Inference\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialisation function of the class\n","        \"\"\"\n","        self._lens = Lens()\n","        self._processor = LensProcessor()\n","\n","        # regex to remove the <pad> and </s> from the output of the LLMs\n","        self._remWord1 = re.compile('(\\s*)<pad>(\\s*)')\n","        self._remWord2 = re.compile('(\\s*)</s>(\\s*)')\n","\n","    ####################################################################################################################\n","\n","    def __call__(\n","            self,\n","            imageFrame,\n","            question\n","    ):\n","        \"\"\"\n","        function to be called to run the inference\n","        :param imageFrame: List of image to be inferred\n","        :param question: List of questions to be passed into the network\n","        :return: [lLMOutput, ImageCaption] : Output of LLM (LENS) + image caption\n","        \"\"\"\n","\n","        # inferring the initial vision models like BLIP, CLIP\n","\n","        samples = self._processor(imageFrame, question)\n","        outputInit = self._lens(samples)\n","\n","        # feeding the output of the vision models to a frozen LLM\n","        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", truncation_side='left', padding=True)\n","        lLMModel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n","        inputIds = tokenizer(samples[\"prompts\"], return_tensors=\"pt\").input_ids\n","        outputLLM = lLMModel.generate(inputIds)\n","        lLMOutput = str(tokenizer.decode(outputLLM[0]))\n","\n","        # use regex to remove unnecessary prefix and suffix\n","        lLMOutput = self._remWord1.sub('', lLMOutput)\n","        lLMOutput = self._remWord2.sub('', lLMOutput)\n","\n","        return [lLMOutput, outputInit[\"caption\"][0]]\n","\n","\n","########################################################################################################################\n","\n","class BLIPVQAInference:\n","    \"\"\"\n","    class for running BLIP_VQA inference\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialisation function of the class\n","        \"\"\"\n","        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        # Loading the BLIP VQA model\n","        self._modelBLIPVQA, \\\n","            self._visProcessors, \\\n","            self._txtProcessors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\",\n","                                                            is_eval=True, device=str(self._device))\n","\n","    ################################################################################################################\n","\n","    def __call__(\n","            self,\n","            imageFrame,\n","            question):\n","        \"\"\"\n","        function to be called to run the inference\n","        :param imageFrame: Image to be inferred\n","        :param question: Question to be passed into the network\n","        :return: outputBLIPVQA : Output of the BLIP_VQA\n","        \"\"\"\n","\n","        imageBLIPVQA = self._visProcessors[\"eval\"](imageFrame).unsqueeze(0).to(self._device)\n","\n","        questionBLIPVQA = self._txtProcessors[\"eval\"](question)\n","\n","        # blip VQA output\n","        bLIPVQAOutput = self._modelBLIPVQA.predict_answers(\n","            samples={\"image\": imageBLIPVQA, \"text_input\": questionBLIPVQA},\n","            inference_method=\"generate\")\n","\n","        return bLIPVQAOutput[0]\n","\n","########################################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpJmJkRBHeSi"},"outputs":[],"source":["\"\"\"\n","This code is written and maintained by Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","import torch\n","import gradio as gr\n","import decord\n","import numpy as np\n","from PIL import Image\n","#from . import dirPath\n","\n","\n","########################################################################################################################\n","class DemoGUIGradio:\n","\n","    ########################################## - INITIALIZE - ##########################################################\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialisation function\n","        \"\"\"\n","        # Load the first model\n","        self._model1 = LENSInference()\n","\n","        # Loading the second model\n","        self._model2 = BLIPVQAInference()\n","\n","    ######################################- GRADIO BACKEND FUNCTIONS - #################################################\n","\n","    @staticmethod\n","    def _getFrameIds(startFrame,\n","                     endFrame,\n","                     numSegments=32,\n","                     jitter=True):\n","\n","        segSize = float(endFrame - startFrame - 1) / numSegments\n","        seq = []\n","\n","        for i in range(numSegments):\n","\n","            start = int(np.round(segSize * i) + startFrame)\n","            end = int(np.round(segSize * (i + 1)) + startFrame)\n","            end = min(end, endFrame)\n","            if jitter:\n","                frameId = np.random.randint(low=start, high=(end + 1))\n","            else:\n","                frameId = (start + end) // 2\n","            seq.append(frameId)\n","\n","        return seq\n","\n","    ####################################################################################################################\n","\n","    def _imageInference(self,\n","                        imageFrame,\n","                        question):\n","        \"\"\"\n","        Function to run when you click on the submit button\n","\n","        :param imageFrame: Image uploaded by the user\n","        :param question: question asked by the user\n","        :return: [LENS Answer, BLIP VQA Answer, Image Description]\n","\n","        \"\"\"\n","\n","        if question == \"\":\n","            question = \"What is the sentiment expressed in the image?\"\n","\n","        model1Output = [\"\", \"\"]\n","\n","        with torch.no_grad():\n","\n","            # inference of Model 2\n","            model2Output = self._model2(imageFrame, question)\n","\n","            # inference of Model 1\n","            model1Output = self._model1([imageFrame], [question])\n","\n","        return [str(model1Output[0]), str(model2Output), str(model1Output[1])]\n","\n","    ######################################- GRADIO BACKEND FUNCTIONS - #################################################\n","\n","    def _videoInference(self,\n","                        videoFile,\n","                        question,\n","                        numSegments=4,\n","                        strideSize=16):\n","        \"\"\"\n","        Function to run when you click on the submit button\n","        :param videoFile: Video uploaded by the user\n","        :param question: question asked by the user\n","        :param numSegments:\n","        :param strideSize:\n","        :return: [LENS Answer, BLIP VQA Answer, Image Description]\n","        \"\"\"\n","\n","        if question == \"\":\n","            question = \"What is the sentiment expressed in the image?\"\n","\n","        if numSegments == \"\":\n","          numSegments = 4\n","\n","        if strideSize ==\"\":\n","          strideSize = 16\n","\n","        model1Output = \"\"\n","        model2Output = \"\"\n","        imageDescription = \"\"\n","\n","        inputVideo = decord.VideoReader(videoFile)\n","        frameSampleSize = int(numSegments) * int(strideSize)\n","        maxStartFrame = len(inputVideo) - frameSampleSize\n","        fps = inputVideo.get_avg_fps()\n","        currFrame = 0\n","\n","        while currFrame == 0 or currFrame < maxStartFrame:\n","\n","            stopFrame = min(currFrame + frameSampleSize, len(inputVideo))\n","            currSec, stopSec = currFrame / fps, stopFrame / fps\n","            frameIds = self._getFrameIds(currFrame, stopFrame, numSegments=numSegments, jitter=False)\n","            frames = inputVideo.get_batch(frameIds).asnumpy()\n","            textToAdd = f\"{'-' * 30} Predictions From: {currSec:2.3f}-{stopSec:2.3f} seconds {'-' * 30}\\n\"\n","            model1Output += textToAdd\n","            model2Output += textToAdd\n","            imageDescription += textToAdd\n","            i = 0\n","\n","            for frame in frames:\n","\n","                imageFrame = Image.fromarray(frame, \"RGB\")\n","\n","                with torch.no_grad():\n","\n","                    # inference of model 2\n","                    _model2Output = self._model2(imageFrame, question)\n","                    textToAdd = f\"Frame {frameIds[i]}: {_model2Output}\\n\"\n","                    model2Output += textToAdd\n","\n","                    # inference of Model 1\n","                    _model1Output = self._model1([imageFrame], [question])\n","                    textToAdd = f\"Frame {frameIds[i]}: {_model1Output[0]}\\n\"\n","                    model1Output += textToAdd\n","                    textToAdd = f\"Frame {frameIds[i]}: {_model1Output[1]}\\n\"\n","                    imageDescription += textToAdd\n","\n","                i += 1\n","            currFrame += frameSampleSize\n","\n","        return [str(model1Output), str(model2Output), str(imageDescription)]\n","        #return model2Output\n","\n","    ######################################- GRADIO FRONTEND FUNCTIONS - ################################################\n","\n","    def __call__(self):\n","        \"\"\"\n","        Function that creates the frontend Gradio interface\n","        \"\"\"\n","\n","        # Creating a block for the app\n","        with gr.Blocks(title=\"Demo for Sentiment Detection using Multimodal LLMs\") as interface:\n","\n","            # title of the demo\n","            # title = \"Demo for Sentiment Detection using Multimodal LLMs\"\n","\n","            # description of the demo\n","            description = \"Gradio initial demo for a proposed Sentiment/Emotion Detection using Multimodal LLM\"\n","\n","            # example Files\n","\n","            \"\"\"exampleInputs = [[f\"{dirPath}/Examples/Crying1.jpeg\", \"What is the facial expression of the person in the \"\n","                                                                  \"image?\"],\n","                             [f\"{dirPath}/Examples/Crying2.jpeg\", \"Describe the emotion expressed in the image\"],\n","                             [f\"{dirPath}/Examples/Happy1.jpeg\", \"What is the person expressing in the image?\"],\n","                             [f\"{dirPath}/Examples/Happy2.jpeg\", \"What is the sentiment expressed in the image?\"],\n","                             [f\"{dirPath}/Examples/SadGirl.jpeg\", \"What is the person doing in the image?\"]]\n","\n","            exampleVideoInputs = [[f\"{dirPath}/Examples/Example1.mp4\",\n","                                   \"What is the facial expression of the person in the \"\n","                                                                  \"image?\"]]\"\"\"\n","\n","            # Setting up Markdown for Title and Description\n","            # gr.Markdown(value=f\"# <p style=\\\"text-align: center;\\\"> {title} </p>\")\n","            gr.Markdown(value=f\"#### {description}\")\n","\n","            with gr.Tab(\"Image File\"):\n","                # Image Tab\n","\n","                with gr.Row():\n","\n","                    with gr.Column():\n","\n","                        # The input components list\n","                        inputs = [gr.Image(type='pil', interactive=True),\n","                                  gr.Textbox(lines=2, label=\"Question\",\n","                                             placeholder=\"Type your question here (Default question:\"\n","                                                         \" What is the sentiment expressed in the image?)...\")]\n","                        with gr.Row():\n","\n","                            # The clear and the submit button objects\n","                            clearButton = gr.ClearButton()\n","                            submitButton = gr.Button(value=\"Submit\", variant=\"primary\")\n","\n","                    with gr.Column():\n","\n","                        # The output components list\n","                        outputs = [gr.Textbox(label=\"Model 1 Answer\"),\n","                                   gr.Textbox(label=\"Model 2 Answer\"),\n","                                   gr.Textbox(label=\"Image Description\")]\n","\n","                # Adding components for the clear Button to clear when it is clicked\n","                clearButton.add(components=inputs + outputs)\n","\n","                # Adding the details for the submit button click action\n","                submitButton.click(fn=self._imageInference, inputs=inputs, outputs=outputs)\n","\n","                # setting up examples\n","                #examples = gr.Examples(examples=exampleInputs, inputs=inputs, outputs=outputs, fn=self._imageInference,\n","                #                       cache_examples=True)\n","\n","            with gr.Tab(\"Video File\"):\n","\n","                with gr.Row():\n","\n","                    with gr.Column():\n","\n","                        # The input components list\n","                        inputs = [gr.Video(label=\"Video File\"),\n","                                  gr.Textbox(lines=2, label=\"Question\",\n","                                             placeholder=\"Type your question here (Default question:\"\n","                                                         \" What is the sentiment expressed in the image?)...\")]\n","                        with gr.Row():\n","\n","                            # the secondary inputs\n","                            inputs2 = [gr.Textbox(label=\"Number of Segments\",\n","                                                  placeholder=\"Enter an integer value (Default: 4)\"),\n","                                       gr.Textbox(label=\"Stride Size\",\n","                                                  placeholder=\"Enter an integer value (Default: 16)\")]\n","                        with gr.Row():\n","\n","                            # The clear and the submit button objects\n","                            clearButton = gr.ClearButton()\n","                            submitButton = gr.Button(value=\"Submit\", variant=\"primary\")\n","\n","                    with gr.Column():\n","\n","                        # The output components list\n","                        outputs = [gr.Textbox(label=\"Model 1 Answer\", max_lines=5),\n","                                   gr.Textbox(label=\"Model 2 Answer\", max_lines=5),\n","                                   gr.Textbox(label=\"Description\", max_lines=5)]\n","                        #outputs = gr.Textbox(label=\"Image Description\")\n","\n","                    # Adding components for the clear Button to clear when it is clicked\n","                clearButton.add(components=inputs + outputs)\n","\n","                # Adding the details for the submit button click action\n","                submitButton.click(fn=self._videoInference, inputs=inputs + inputs2, outputs=outputs)\n","\n","                # setting up examples\n","                #examples = gr.Examples(examples=exampleVideoInputs, inputs=inputs, outputs=outputs,\n","                #                       fn=self._videoInference,\n","                #                       cache_examples=True)\n","\n","        # Launch interface\n","        interface.launch(share=True, debug=True)\n","\n","########################################################################################################################\n","\n","# Load gradio class\n","gradioUI = DemoGUIGradio()\n","\n","\n","######################################- GRADIO UI DEPLOYMENT - #####################################################\n","\n","# deploy UI\n","gradioUI()\n","\n","########################################################################################################################\n"]},{"cell_type":"markdown","metadata":{"id":"F0fxNgz24faA"},"source":["#### Gradio Chatbot UI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmpvqE6alI6A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAoGsq2F4jN4"},"outputs":[],"source":["# @title MPLUG-OWL (a lighter LLaVA for testing purposes)\n","\"\"\"\n","This code is written and maintained by Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","\n","# code to mount my drive\n","!nvidia-smi\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/mPLUG-Owl\n","\n","import os\n","import argparse\n","import datetime\n","import json\n","import os\n","import time\n","import torch\n","import gradio as gr\n","import requests\n","\n","from serve.conversation import default_conversation\n","from serve.gradio_css import code_highlight_css\n","from serve.gradio_patch import Chatbot as grChatbot\n","from serve.serve_utils import (\n","    add_text, after_process_image, disable_btn, no_change_btn,\n","    downvote_last_response, enable_btn, flag_last_response,\n","    get_window_url_params, init, regenerate, upvote_last_response,\n",")\n","from serve.model_worker import mPLUG_Owl_Server\n","from serve.model_utils import post_process_code\n","\n","########################################################################################################################\n","class DemoGUIGradio:\n","\n","    @staticmethod\n","    def loadInterface(state, request: gr.Request):\n","\n","      state = default_conversation.copy()\n","\n","      return (state)\n","\n","    ####################################################################################################################\n","\n","    @staticmethod\n","    def clearHistory(state, request: gr.Request):\n","\n","      state = default_conversation.copy()\n","\n","      return (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","    ####################################################################################################################\n","\n","    @staticmethod\n","    def addTextHttpBot( state, text, image, video, max_output_tokens, temperature, top_k, top_p,\n","                       num_beams, no_repeat_ngram_size, length_penalty,\n","                        do_sample, request: gr.Request):\n","\n","      if len(text) <= 0 and (image is None or video is None):\n","\n","        state.skip_next = True\n","        return (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","      if image is not None:\n","\n","        if '<image>' not in text:\n","\n","            text = text + '\\n<image>'\n","\n","        text = (text, image)\n","\n","      if video is not None:\n","\n","        num_frames = 4\n","\n","        if '<image>' not in text:\n","\n","          text = text + '\\n<image>' * num_frames\n","\n","        text = (text, video)\n","\n","      state.append_message(state.roles[0], text)\n","      state.append_message(state.roles[1], None)\n","      state.skip_next = False\n","\n","      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","      if state.skip_next:\n","        # This generate call is skipped due to invalid inputs\n","        yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","        return\n","\n","      prompt = after_process_image(state.get_prompt())\n","      images = state.get_images()\n","\n","      data = {\n","          \"text_input\": prompt,\n","          \"images\": images if len(images) > 0 else [],\n","          \"generation_config\": {\n","              \"top_k\": int(top_k),\n","              \"top_p\": float(top_p),\n","              \"num_beams\": int(num_beams),\n","              \"no_repeat_ngram_size\": int(no_repeat_ngram_size),\n","              \"length_penalty\": float(length_penalty),\n","              \"do_sample\": bool(do_sample),\n","              \"temperature\": float(temperature),\n","              \"max_new_tokens\": min(int(max_output_tokens), 1536),\n","              }\n","          }\n","      state.messages[-1][-1] = \"▌\"\n","      yield (state, state.to_gradio_chatbot(), \"\",None, None)\n","\n","      try:\n","          for chunk in model.predict(data):\n","              if chunk:\n","                  if chunk[1]:\n","                      output = chunk[0].strip()\n","                      output = post_process_code(output)\n","                      state.messages[-1][-1] = output + \"▌\"\n","                      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","                  else:\n","                      output = chunk[0].strip()\n","                      state.messages[-1][-1] = output\n","                      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","                      return\n","                  time.sleep(0.03)\n","\n","      except requests.exceptions.RequestException as e:\n","          state.messages[-1][-1] = \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n","          yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","          return\n","\n","      state.messages[-1][-1] = state.messages[-1][-1][:-1]\n","      yield (state, state.to_gradio_chatbot(), \"\", None, None)\n","\n","    ######################################- GRADIO FRONTEND FUNCTIONS - ################################################\n","\n","    def __call__(self):\n","        \"\"\"\n","        Function that creates the frontend Gradio interface\n","        \"\"\"\n","\n","        # Creating a block for the app\n","        with gr.Blocks(title=\"Demo for Sentiment Detection using multimodal LLMs\") as interface:\n","\n","            state = gr.State()\n","\n","            # title of the demo\n","            title = \"Demo for Sentiment Detection using Multimodal LLMs\"\n","\n","            # description of the demo\n","            description = \"Gradio initial demo for a proposed Sentiment/Emotion Detection using Multimodal LLM\"\n","\n","            # Setting up Markdown for Title and Description\n","            gr.Markdown(value=f\"# <p style=\\\"text-align: center;\\\"> {title} </p>\")\n","            gr.Markdown(value=f\"#### {description}\")\n","\n","            with gr.Row():\n","              with gr.Column():\n","\n","                imagebox = gr.Image(type=\"pil\", visible=True)\n","                videobox = gr.Video()\n","\n","                with gr.Accordion(\"Parameters\", open=True, visible=True) as parameterRow:\n","\n","                    max_output_tokens = gr.Slider(minimum=0, maximum=1024, value=512, step=64, interactive=True, label=\"Max output tokens\",)\n","                    temperature = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, interactive=True, label=\"Temperature\",)\n","                    top_k = gr.Slider(minimum=1, maximum=5, value=3, step=1, interactive=True, label=\"Top K\",)\n","                    top_p = gr.Slider(minimum=0, maximum=1, value=0.9, step=0.1, interactive=True, label=\"Top p\",)\n","                    length_penalty = gr.Slider(minimum=1, maximum=5, value=1, step=0.1, interactive=True, label=\"length_penalty\",)\n","                    num_beams = gr.Slider(minimum=1, maximum=5, value=1, step=1, interactive=True, label=\"Beam Size\",)\n","                    no_repeat_ngram_size = gr.Slider(minimum=1, maximum=5, value=2, step=1, interactive=True, label=\"no_repeat_ngram_size\",)\n","                    do_sample = gr.Checkbox(interactive=True, value=True, label=\"do_sample\")\n","\n","              with gr.Column():\n","                chatbot = grChatbot(elem_id=\"chatbot\", visible=True).style(height=800)\n","                with gr.Row():\n","                  with gr.Column(scale=8):\n","                    textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", visible=True).style(container=False)\n","                  with gr.Column(scale=1, min_width=60):\n","                    submitBtn = gr.Button(value=\"Submit\", visible=True)\n","                with gr.Row(visible=True) as buttonRow:\n","                  clearBtn = gr.Button(value=\"🗑️  Clear history\", interactive=True)\n","\n","            btnList = [clearBtn]\n","\n","            parameter_list = [max_output_tokens, temperature, top_k,\n","                              top_p, num_beams, no_repeat_ngram_size, length_penalty, do_sample]\n","\n","            clearBtn.click(self.clearHistory, None, [state, chatbot, textbox, videobox])\n","\n","            textbox.submit(self.addTextHttpBot,\n","             [state, textbox, imagebox, videobox] + parameter_list,\n","             [state, chatbot, textbox, imagebox, videobox]\n","                           )\n","\n","            submitBtn.click(self.addTextHttpBot,\n","               [state, textbox, imagebox, videobox] + parameter_list,\n","               [state, chatbot, textbox, imagebox, videobox]\n","             )\n","\n","            interface.load(self.loadInterface, [state], [state])\n","\n","        # Launch interface\n","        #interface.launch(share=True, debug=True)\n","        return interface\n","\n","########################################################################################################################\n","\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","else:\n","  device = \"cpu\"\n","\n","model = mPLUG_Owl_Server(\n","    base_model='MAGAer13/mplug-owl-llama-7b',\n","    load_in_8bit=True,\n","    bf16=True,\n","    device=device,\n","    )\n","\n","# deploy UI\n","gradioUI = DemoGUIGradio()\n","interface = gradioUI()\n","interface.queue(concurrency_count=3, status_update_rate=10, api_open=False).launch(debug=True, share=True)\n","\n","########################################################################################################################\n","\n"]},{"cell_type":"code","source":["# @title LLaVA\n","\"\"\"\n","This code is written and maintained by Aditya Ramanath Poonja\n","huggingFace : https://huggingface.co/pooadi\n","GitHub      : https://github.com/pooadi\n","\"\"\"\n","\n","\n","# code to mount my drive\n","!nvidia-smi\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ChatAI_Project/LLaVA\n","\n","import argparse\n","import datetime\n","import json\n","import os\n","import time\n","\n","import gradio as gr\n","import requests\n","\n","from llava.conversation import (default_conversation, conv_templates,\n","                                   SeparatorStyle)\n","from llava.constants import LOGDIR\n","from llava.utils import (build_logger, server_error_msg,\n","    violates_moderation, moderation_msg)\n","import hashlib\n","\n","################################################################################################\n","\n","\n","logger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n","\n","headers = {\"User-Agent\": \"LLaVA Client\"}\n","\n","no_change_btn = gr.Button.update()\n","enable_btn = gr.Button.update(interactive=True)\n","disable_btn = gr.Button.update(interactive=False)\n","\n","priority = {\n","    \"vicuna-13b\": \"aaaaaaa\",\n","    \"koala-13b\": \"aaaaaab\",\n","}\n","\n","\n","##################################################################################################\n","\n","def get_conv_log_filename():\n","\n","    t = datetime.datetime.now()\n","    name = os.path.join(LOGDIR, f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\")\n","    return name\n","\n","##################################################################################################\n","\n","def get_model_list():\n","    ret = requests.post(args.controller_url + \"/refresh_all_workers\")\n","    assert ret.status_code == 200\n","    ret = requests.post(args.controller_url + \"/list_models\")\n","    models = ret.json()[\"models\"]\n","    models.sort(key=lambda x: priority.get(x, x))\n","    logger.info(f\"Models: {models}\")\n","    return models\n","\n","####################################################################################################\n","\n","get_window_url_params = \"\"\"\n","function() {\n","    const params = new URLSearchParams(window.location.search);\n","    url_params = Object.fromEntries(params);\n","    console.log(url_params);\n","    return url_params;\n","    }\n","\"\"\"\n","\n","####################################################################################################\n","\n","def load_demo(url_params, request: gr.Request):\n","\n","    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n","\n","    dropdown_update = gr.Dropdown.update(visible=True)\n","\n","    if \"model\" in url_params:\n","\n","        model = url_params[\"model\"]\n","\n","        if model in models:\n","\n","            dropdown_update = gr.Dropdown.update(\n","                value=model, visible=True)\n","\n","    state = default_conversation.copy()\n","\n","    return (state,\n","            dropdown_update,\n","            gr.Chatbot.update(visible=True),\n","            gr.Textbox.update(visible=True),\n","            gr.Button.update(visible=True),\n","            gr.Row.update(visible=True),\n","            gr.Accordion.update(visible=True))\n","\n","####################################################################################################\n","\n","def load_demo_refresh_model_list(request: gr.Request):\n","\n","    logger.info(f\"load_demo. ip: {request.client.host}\")\n","\n","    models = get_model_list()\n","    state = default_conversation.copy()\n","\n","    return (state, gr.Dropdown.update(\n","               choices=models,\n","               value=models[0] if len(models) > 0 else \"\"),\n","            gr.Chatbot.update(visible=True),\n","            gr.Textbox.update(visible=True),\n","            gr.Button.update(visible=True),\n","            gr.Row.update(visible=True),\n","            gr.Accordion.update(visible=True))\n","\n","####################################################################################################\n","\n","def clear_history(request: gr.Request):\n","    logger.info(f\"clear_history. ip: {request.client.host}\")\n","    state = default_conversation.copy()\n","    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n","\n","####################################################################################################\n","\n","def add_text(state, text, image, image_process_mode, request: gr.Request):\n","\n","    logger.info(f\"add_text. ip: {request.client.host}. len: {len(text)}\")\n","\n","    if len(text) <= 0 and image is None:\n","\n","        state.skip_next = True\n","        return (state, state.to_gradio_chatbot(), \"\", None) + (no_change_btn,) * 5\n","\n","    if args.moderate:\n","\n","        flagged = violates_moderation(text)\n","\n","        if flagged:\n","\n","            state.skip_next = True\n","            return (state, state.to_gradio_chatbot(), moderation_msg, None) + (\n","                no_change_btn,) * 5\n","\n","    text = text[:1536]  # Hard cut-off\n","\n","    if image is not None:\n","\n","        text = text[:1200]  # Hard cut-off for images\n","\n","        if '<image>' not in text:\n","\n","            # text = '<Image><image></Image>' + text\n","\n","            text = text + '\\n<image>'\n","\n","        text = (text, image, image_process_mode)\n","\n","        if len(state.get_images(return_pil=True)) > 0:\n","\n","            state = default_conversation.copy()\n","\n","    state.append_message(state.roles[0], text)\n","    state.append_message(state.roles[1], None)\n","    state.skip_next = False\n","\n","    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n","\n","####################################################################################################\n","\n","def http_bot(state, model_selector, temperature, top_p, max_new_tokens, request: gr.Request):\n","\n","    logger.info(f\"http_bot. ip: {request.client.host}\")\n","    start_tstamp = time.time()\n","    model_name = model_selector\n","\n","    if state.skip_next:\n","        # This generate call is skipped due to invalid inputs\n","        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5\n","        return\n","\n","    if len(state.messages) == state.offset + 2:\n","\n","        # First round of conversation\n","        if \"llava\" in model_name.lower():\n","            if 'llama-2' in model_name.lower():\n","                template_name = \"llava_llama_2\"\n","            elif \"v1\" in model_name.lower():\n","                if 'mmtag' in model_name.lower():\n","                    template_name = \"v1_mmtag\"\n","                elif 'plain' in model_name.lower() and 'finetune' not in model_name.lower():\n","                    template_name = \"v1_mmtag\"\n","                else:\n","                    template_name = \"llava_v1\"\n","            elif \"mpt\" in model_name.lower():\n","                template_name = \"mpt\"\n","            else:\n","                if 'mmtag' in model_name.lower():\n","                    template_name = \"v0_mmtag\"\n","                elif 'plain' in model_name.lower() and 'finetune' not in model_name.lower():\n","                    template_name = \"v0_mmtag\"\n","                else:\n","                    template_name = \"llava_v0\"\n","        elif \"mpt\" in model_name:\n","            template_name = \"mpt_text\"\n","        elif \"llama-2\" in model_name:\n","            template_name = \"llama_2\"\n","        else:\n","            template_name = \"vicuna_v1\"\n","        new_state = conv_templates[template_name].copy()\n","        new_state.append_message(new_state.roles[0], state.messages[-2][1])\n","        new_state.append_message(new_state.roles[1], None)\n","        state = new_state\n","\n","    # Query worker address\n","    controller_url = args.controller_url\n","    ret = requests.post(controller_url + \"/get_worker_address\",\n","            json={\"model\": model_name})\n","    worker_addr = ret.json()[\"address\"]\n","    logger.info(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n","\n","    # No available worker\n","    if worker_addr == \"\":\n","        state.messages[-1][-1] = server_error_msg\n","        yield (state, state.to_gradio_chatbot(), disable_btn, disable_btn, disable_btn, enable_btn, enable_btn)\n","        return\n","\n","    # Construct prompt\n","    prompt = state.get_prompt()\n","\n","    all_images = state.get_images(return_pil=True)\n","    all_image_hash = [hashlib.md5(image.tobytes()).hexdigest() for image in all_images]\n","    for image, hash in zip(all_images, all_image_hash):\n","        t = datetime.datetime.now()\n","        filename = os.path.join(LOGDIR, \"serve_images\", f\"{t.year}-{t.month:02d}-{t.day:02d}\", f\"{hash}.jpg\")\n","        if not os.path.isfile(filename):\n","            os.makedirs(os.path.dirname(filename), exist_ok=True)\n","            image.save(filename)\n","\n","    # Make requests\n","    pload = {\n","        \"model\": model_name,\n","        \"prompt\": prompt,\n","        \"temperature\": float(temperature),\n","        \"top_p\": float(top_p),\n","        \"max_new_tokens\": min(int(max_new_tokens), 1536),\n","        \"stop\": state.sep if state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else state.sep2,\n","        \"images\": f'List of {len(state.get_images())} images: {all_image_hash}',\n","    }\n","    logger.info(f\"==== request ====\\n{pload}\")\n","\n","    pload['images'] = state.get_images()\n","\n","    state.messages[-1][-1] = \"▌\"\n","    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n","\n","    try:\n","        # Stream output\n","        response = requests.post(worker_addr + \"/worker_generate_stream\",\n","            headers=headers, json=pload, stream=True, timeout=10)\n","        for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n","            if chunk:\n","                data = json.loads(chunk.decode())\n","                if data[\"error_code\"] == 0:\n","                    output = data[\"text\"][len(prompt):].strip()\n","                    state.messages[-1][-1] = output + \"▌\"\n","                    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n","                else:\n","                    output = data[\"text\"] + f\" (error_code: {data['error_code']})\"\n","                    state.messages[-1][-1] = output\n","                    yield (state, state.to_gradio_chatbot()) + (disable_btn, disable_btn, disable_btn, enable_btn, enable_btn)\n","                    return\n","                time.sleep(0.03)\n","    except requests.exceptions.RequestException as e:\n","        state.messages[-1][-1] = server_error_msg\n","        yield (state, state.to_gradio_chatbot()) + (disable_btn, disable_btn, disable_btn, enable_btn, enable_btn)\n","        return\n","\n","    state.messages[-1][-1] = state.messages[-1][-1][:-1]\n","    yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5\n","\n","    finish_tstamp = time.time()\n","    logger.info(f\"{output}\")\n","\n","    with open(get_conv_log_filename(), \"a\") as fout:\n","        data = {\n","            \"tstamp\": round(finish_tstamp, 4),\n","            \"type\": \"chat\",\n","            \"model\": model_name,\n","            \"start\": round(start_tstamp, 4),\n","            \"finish\": round(start_tstamp, 4),\n","            \"state\": state.dict(),\n","            \"images\": all_image_hash,\n","            \"ip\": request.client.host,\n","        }\n","        fout.write(json.dumps(data) + \"\\n\")\n","\n","####################################################################################################\n","\n","title_markdown = (\"\"\"\n","# 🌋 LLaVA: Large Language and Vision Assistant\n","[[Project Page]](https://llava-vl.github.io) [[Paper]](https://arxiv.org/abs/2304.08485) [[Code]](https://github.com/haotian-liu/LLaVA) [[Model]](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0)\n","\"\"\")\n","\n","tos_markdown = (\"\"\"\n","### Terms of use\n","By using this service, users are required to agree to the following terms:\n","The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. The service may collect user dialogue data for future research.\n","Please click the \"Flag\" button if you get any inappropriate answer! We will collect those to keep improving our moderator.\n","For an optimal experience, please use desktop computers for this demo, as mobile devices may compromise its quality.\n","\"\"\")\n","\n","\n","learn_more_markdown = (\"\"\"\n","### License\n","The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.\n","\"\"\")\n","\n","####################################################################################################\n","\n","def build_demo(embed_mode):\n","    textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", visible=False, container=False)\n","    with gr.Blocks(title=\"LLaVA\", theme=gr.themes.Base()) as demo:\n","        state = gr.State()\n","\n","        if not embed_mode:\n","            gr.Markdown(title_markdown)\n","\n","        with gr.Row():\n","            with gr.Column(scale=3):\n","                with gr.Row(elem_id=\"model_selector_row\"):\n","                    model_selector = gr.Dropdown(\n","                        choices=models,\n","                        value=models[0] if len(models) > 0 else \"\",\n","                        interactive=True,\n","                        show_label=False,\n","                        container=False)\n","\n","                imagebox = gr.Image(type=\"pil\")\n","                image_process_mode = gr.Radio(\n","                    [\"Crop\", \"Resize\", \"Pad\"],\n","                    value=\"Crop\",\n","                    label=\"Preprocess for non-square image\")\n","\n","                cur_dir = os.path.dirname(os.path.abspath(__file__))\n","                gr.Examples(examples=[\n","                    [f\"{cur_dir}/examples/extreme_ironing.jpg\", \"What is unusual about this image?\"],\n","                    [f\"{cur_dir}/examples/waterview.jpg\", \"What are the things I should be cautious about when I visit here?\"],\n","                ], inputs=[imagebox, textbox])\n","\n","                with gr.Accordion(\"Parameters\", open=False, visible=False) as parameter_row:\n","                    temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.2, step=0.1, interactive=True, label=\"Temperature\",)\n","                    top_p = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.1, interactive=True, label=\"Top P\",)\n","                    max_output_tokens = gr.Slider(minimum=0, maximum=1024, value=512, step=64, interactive=True, label=\"Max output tokens\",)\n","\n","            with gr.Column(scale=6):\n","                chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"LLaVA Chatbot\", visible=False, height=550)\n","                with gr.Row():\n","                    with gr.Column(scale=8):\n","                        textbox.render()\n","                    with gr.Column(scale=1, min_width=60):\n","                        submit_btn = gr.Button(value=\"Submit\", visible=False)\n","                with gr.Row(visible=False) as button_row:\n","                    upvote_btn = gr.Button(value=\"👍  Upvote\", interactive=False)\n","                    downvote_btn = gr.Button(value=\"👎  Downvote\", interactive=False)\n","                    flag_btn = gr.Button(value=\"⚠️  Flag\", interactive=False)\n","                    #stop_btn = gr.Button(value=\"⏹️  Stop Generation\", interactive=False)\n","                    regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n","                    clear_btn = gr.Button(value=\"🗑️  Clear history\", interactive=False)\n","\n","        if not embed_mode:\n","            gr.Markdown(tos_markdown)\n","            gr.Markdown(learn_more_markdown)\n","        url_params = gr.JSON(visible=False)\n","\n","        # Register listeners\n","        btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]\n","        upvote_btn.click(upvote_last_response,\n","            [state, model_selector], [textbox, upvote_btn, downvote_btn, flag_btn])\n","        downvote_btn.click(downvote_last_response,\n","            [state, model_selector], [textbox, upvote_btn, downvote_btn, flag_btn])\n","        flag_btn.click(flag_last_response,\n","            [state, model_selector], [textbox, upvote_btn, downvote_btn, flag_btn])\n","        regenerate_btn.click(regenerate, [state, image_process_mode],\n","            [state, chatbot, textbox, imagebox] + btn_list).then(\n","            http_bot, [state, model_selector, temperature, top_p, max_output_tokens],\n","            [state, chatbot] + btn_list)\n","        clear_btn.click(clear_history, None, [state, chatbot, textbox, imagebox] + btn_list)\n","\n","        textbox.submit(add_text, [state, textbox, imagebox, image_process_mode], [state, chatbot, textbox, imagebox] + btn_list\n","            ).then(http_bot, [state, model_selector, temperature, top_p, max_output_tokens],\n","                   [state, chatbot] + btn_list)\n","        submit_btn.click(add_text, [state, textbox, imagebox, image_process_mode], [state, chatbot, textbox, imagebox] + btn_list\n","            ).then(http_bot, [state, model_selector, temperature, top_p, max_output_tokens],\n","                   [state, chatbot] + btn_list)\n","\n","        if args.model_list_mode == \"once\":\n","            demo.load(load_demo, [url_params], [state, model_selector,\n","                chatbot, textbox, submit_btn, button_row, parameter_row],\n","                _js=get_window_url_params)\n","        elif args.model_list_mode == \"reload\":\n","            demo.load(load_demo_refresh_model_list, None, [state, model_selector,\n","                chatbot, textbox, submit_btn, button_row, parameter_row])\n","        else:\n","            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n","\n","    return demo\n","\n","####################################################################################################\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n","    parser.add_argument(\"--port\", type=int)\n","    parser.add_argument(\"--controller-url\", type=str, default=\"http://localhost:21001\")\n","    parser.add_argument(\"--concurrency-count\", type=int, default=8)\n","    parser.add_argument(\"--model-list-mode\", type=str, default=\"once\",\n","        choices=[\"once\", \"reload\"])\n","    parser.add_argument(\"--share\", action=\"store_true\")\n","    parser.add_argument(\"--moderate\", action=\"store_true\")\n","    parser.add_argument(\"--embed\", action=\"store_true\")\n","    args = parser.parse_args()\n","    logger.info(f\"args: {args}\")\n","\n","    models = get_model_list()\n","\n","    logger.info(args)\n","    demo = build_demo(args.embed)\n","    demo.queue(concurrency_count=args.concurrency_count, status_update_rate=10,\n","               api_open=False).launch(\n","        server_name=args.host, server_port=args.port, share=args.share)\n","\n"],"metadata":{"id":"UfnZBQaY3NbO"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["l4drlTkXyhOn"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}